核心实现流程说明
1. 语音对话流程
麦克风 → VAD检测 → ASR识别 → LLM推理 → TTS合成 → 扬声器输出
         ↓         ↓        ↓        ↓
      打断处理   文本输入  工具调用  表情控制
2. 视觉感知流程
摄像头/屏幕 → 图像捕获 → 视觉描述 → LLM上下文增强
3. Live2D 表情控制
LLM响应 → 情感分析 → 表情映射 → Live2D模型渲染 → 前端展示
4. 配置驱动架构
conf.yaml → ConfigManager → ServiceContext → 各引擎初始化
                    ↓
              配置热切换支持
5. 工具扩展机制 (MCP)
ServerRegistry → ToolAdapter → ToolManager → ToolExecutor → MCPClient
                                                    ↓
                                              工具调用与结果返回
关键设计特点
模块化设计：ASR/TTS/VAD/LLM 均通过工厂模式可插拔
上下文隔离：每个 WebSocket 连接拥有独立的 ServiceContext
配置热切换：支持运行时切换角色配置
离线优先：所有模型可本地运行，隐私安全
跨平台：支持 Windows/macOS/Linux，CPU/GPU 灵活切换

