# Open-LLM-VTuber 启动部署流程（Windows + Conda）

## 1) 环境准备
```powershell
conda create -n open-llm-vtuber python=3.11 -y
conda activate open-llm-vtuber
pip install uv
```

## 2) 克隆与依赖安装
```powershell
git clone https://github.com/Open-LLM-VTuber/Open-LLM-VTuber.git
cd Open-LLM-VTuber
git submodule update --init --recursive
uv sync
```

## 3) 配置文件生成（Ollama + Sherpa-onnx）
```powershell
copy config_templates\conf.default.yaml conf.yaml
```

编辑 `conf.yaml` 关键项（保持其余默认即可）：

```yaml
system_config:
  host: 'localhost'
  port: 12393

character_config:
  agent_config:
    agent_settings:
      basic_memory_agent:
        llm_provider: 'ollama_llm'
    llm_configs:
      ollama_llm:
        base_url: 'http://10.10.50.115:11434/v1'
        model: 'hf.co/unsloth/GLM-4.7-Flash-GGUF:Q8_0'

  asr_config:
    asr_model: 'sherpa_onnx_asr'
```

> 说明：`sherpa_onnx_asr` 支持 CPU 推理，无需显卡。

## 4) Sherpa-onnx 模型准备（CPU 可用）
推荐使用 SenseVoice 模型（官方默认）：

```yaml
sherpa_onnx_asr:
  model_type: 'sense_voice'
  sense_voice: 'models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/model.onnx'
  tokens: 'models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/tokens.txt'
  provider: 'cpu'
```

模型下载地址（任选其一）：
- https://github.com/k2-fsa/sherpa-onnx/releases/tag/asr-models

下载后把模型文件放入 `models/sherpa-onnx-sense-voice-zh-en-ja-ko-yue-2024-07-17/` 目录。

## 5) 启动服务器
```powershell
conda run -n open-llm-vtuber uv run run_server.py
```

浏览器访问：
```
http://localhost:12393
```

## 6) 常见问题处理
### 端口占用（12393）
```powershell
netstat -ano | findstr :12393
taskkill /F /PID <PID>
```

### ffmpeg 警告
这是音频依赖提示，不会阻止启动。后续可安装：
```powershell
choco install ffmpeg
```

---

# 核心实现流程说明
1. 语音对话流程
麦克风 → VAD检测 → ASR识别 → LLM推理 → TTS合成 → 扬声器输出
         ↓         ↓        ↓        ↓
      打断处理   文本输入  工具调用  表情控制
2. 视觉感知流程
摄像头/屏幕 → 图像捕获 → 视觉描述 → LLM上下文增强
3. Live2D 表情控制
LLM响应 → 情感分析 → 表情映射 → Live2D模型渲染 → 前端展示
4. 配置驱动架构
conf.yaml → ConfigManager → ServiceContext → 各引擎初始化
                    ↓
              配置热切换支持
5. 工具扩展机制 (MCP)
ServerRegistry → ToolAdapter → ToolManager → ToolExecutor → MCPClient
                                                    ↓
                                              工具调用与结果返回
关键设计特点
模块化设计：ASR/TTS/VAD/LLM 均通过工厂模式可插拔
上下文隔离：每个 WebSocket 连接拥有独立的 ServiceContext
配置热切换：支持运行时切换角色配置
离线优先：所有模型可本地运行，隐私安全
跨平台：支持 Windows/macOS/Linux，CPU/GPU 灵活切换
